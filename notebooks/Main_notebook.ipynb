{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b241f299",
   "metadata": {},
   "source": [
    "# Etude de cas de l'utilisation des IA Génératives (LLMs) dans l'enseignement supérieur de la science du numérique.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a41478",
   "metadata": {},
   "source": [
    "## Préambule\n",
    "\n",
    "Les IA génératives telles que ChatGPT, DeepSeek, ou Claude Sonnet sont indéniablement devenues ces dernières années des outils très prominents dans le quotidien numérique des étudiants. Étant des outils très puissants, il est essentiel de savoir s'en servir convenablement pour en tirer le maximum. Cela demande en premier lieu d'*au moins* comprendre dans les très (très) grandes lignes leur principe de fonctionnement et ce que la technologie implique, mais aussi leurs potentiel et limites actuelles. Cela évitera notamment ce genre de remarques:\n",
    "\n",
    "<div>\n",
    "    <center>\n",
    "    <img src=\"img/illustrative_tweet.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principe de l'IA Générative\n",
    "\n",
    "Pour faire très (très) simple en omettant toutes les maths derrière:\n",
    "\n",
    "&rarr; un **modèle de langage** comme **GPT 4.1** (≠ ChatGPT!) va, à partir d'un texte en entrée (un contexte en fait), découper ce texte en morceaux/~mots appelés *tokens* et leur attribuer un vecteur dans un espace de très grande dimension (beaucoup d'axes). \n",
    "\n",
    "&rarr; Ce vecteur déterminera d'une certaine manière le sens \"*sémantique*\" d'un token, et ainsi les autres mots de sens similaires auront des vecteurs proches dans l'espace. Les valeurs qui caractérisent ces vecteurs sont choisies aléatoirement puis apprises lors de l'**apprentissage** du modèle; une étape antérieure où on apprend au modèle à comparer et donc catégoriser les entrées qu'on lui donne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1804d558",
   "metadata": {},
   "source": [
    "Plus bas il y a par exemple un code en Python que vous pouvez exécuter (pas besoin de chercher à le comprendre, il faut juste cliquer sur le bouton latéral gauche pour démarrer la zone de code) afin d'observer l'angle entre des vecteurs de tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fed032",
   "metadata": {},
   "outputs": [],
   "source": [
    "## On installe une librairie qui permet de télécharger de larges modèles utilisés dans le traîtement du \n",
    "# langage naturel\n",
    "\n",
    "#à ne lancer qu'une fois et à installer sur Python <=3.12 !\n",
    "pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a21e9d4",
   "metadata": {},
   "source": [
    "On utilise le modèle ***glove-wiki-gigaword-50***, qui n'est pas un LLM mais un *Word Embedding Model*. Il s'agit dans tous les cas exactement de ce qu'on a décrit dans le paragraphe précédent: C'est un modèle qui capture les relations sémantiques entre les mots et peut les représenter dans des vecteurs spaciaux qui symbolisent leur proximité sémantique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d12ca0",
   "metadata": {},
   "source": [
    "On importe *gensim* et on télécharge le modèle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc14c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader # On importe la librairie qui nous permet de télécharger des modèles de \n",
    "# plongement de mots\n",
    "\n",
    "model = gensim.downloader.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf127b25",
   "metadata": {},
   "source": [
    "On va observer la proximité sémantique des tokens en regardant l'angle que font leur vecteur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1a528ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le vecteur normalisé du mot 'tower' est:\n",
      " [ 2.32295305e-01  2.39117995e-01  1.50941327e-01 -1.19652124e-02\n",
      "  1.02237053e-01 -1.42626569e-01 -6.50604963e-02 -9.18937027e-02\n",
      " -9.26488563e-02 -1.52530596e-01 -6.78442419e-02 -5.05627971e-03\n",
      " -1.01615526e-01  1.29110754e-01 -1.68155968e-01  1.69180378e-01\n",
      " -5.00080697e-02  6.56374916e-02 -2.24784255e-01 -4.31934837e-03\n",
      "  1.39120072e-01 -7.96452612e-02 -3.31821531e-01 -1.02224916e-01\n",
      " -3.37773636e-02 -1.36957854e-01 -6.43762052e-02  1.79177538e-01\n",
      " -6.38781721e-03 -3.15989628e-02  4.00959432e-01 -2.40312472e-01\n",
      "  1.68728903e-01 -3.71887088e-02 -5.40368967e-02  2.35230885e-02\n",
      "  2.23164648e-01 -7.27557614e-02  5.06437756e-03 -8.22265446e-03\n",
      "  6.21147975e-02 -8.31598490e-02  1.71247441e-02  4.55014519e-02\n",
      " -1.03160247e-01  1.33252963e-01 -2.51690354e-02 -2.84224659e-01\n",
      "  3.27529488e-05 -1.06346868e-01]\n",
      "\n",
      "Le vecteur normalisé du mot 'building' est:\n",
      " [ 0.17743999  0.09454948  0.07480241 -0.08101078  0.09619492 -0.07971676\n",
      " -0.16223752 -0.13204496 -0.05950845 -0.10866108 -0.11274996 -0.10573626\n",
      " -0.10513775  0.01000806 -0.10548002  0.08904761  0.01905522  0.14769033\n",
      "  0.01317686 -0.04613813  0.2347539   0.03047079 -0.30148652 -0.134018\n",
      " -0.074343   -0.19737923 -0.0099387   0.0777199   0.05390042  0.00715683\n",
      "  0.61399144 -0.17508988  0.03068676 -0.16171041 -0.01307216  0.11846781\n",
      "  0.12289712  0.07946052  0.0758713   0.07931043 -0.06971419 -0.18740413\n",
      "  0.02682483  0.00874955 -0.01437277  0.09863288 -0.1464805  -0.13456526\n",
      " -0.00523502 -0.05424635]\n",
      "\n",
      "L'angle entre les deux vecteurs en degrés est environ 38.068\n"
     ]
    }
   ],
   "source": [
    "# On va utiliser les fonctions intégrées à Numpy pour traiter nos vecteurs\n",
    "import numpy as np\n",
    "\n",
    "## Fonctions utilitaires\n",
    "def normaliser(vecteur):\n",
    "    \"\"\" Normalise le vecteur en argument.\n",
    "    \"\"\"\n",
    "    return vecteur / np.linalg.norm(vecteur)\n",
    "\n",
    "def angle(v1, v2):\n",
    "    \"\"\"Retourne l'angle fait par les vecteurs donnés en argument.\n",
    "    \"\"\"\n",
    "    v1_u = normaliser(v1)\n",
    "    v2_u = normaliser(v2)\n",
    "    res = np.dot(v1_u, v2_u)\n",
    "    res = np.clip(res, -1.0, 1.0)\n",
    "    res = np.arccos(res)\n",
    "    res *= 180/np.pi\n",
    "    return round(res, 3)\n",
    "\n",
    "## Affichage des vecteurs\n",
    "\n",
    "#Exemple avec les vecteurs des mots \"tour\" et \"bâtiment\" \n",
    "vecteur_jeu = model[\"tower\"] \n",
    "\n",
    "vecteur_player = model[\"building\"]\n",
    "\n",
    "print(f\"Le vecteur normalisé du mot 'tower' est:\\n {normaliser(vecteur_jeu)}\\n\")\n",
    "print(f\"Le vecteur normalisé du mot 'building' est:\\n {normaliser(vecteur_player)}\\n\")\n",
    "print(f\"L'angle entre les deux vecteurs en degrés est environ {angle(vecteur_jeu, vecteur_player)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec3471b",
   "metadata": {},
   "source": [
    "Voici un autre exemple avec des mots sans grande liaison sémantique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06940295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'angle entre les deux vecteurs en degrés est environ 105.659\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vecteur_jeu = model[\"printer\"] # veccteur pour imprimante\n",
    "\n",
    "vecteur_player = model[\"platypus\"] # vecteur pour ornythorinque\n",
    "\n",
    "# print(f\"Le vecteur normalisé du mot 'game' est:\\n {normaliser(vecteur_jeu)}\\n\")\n",
    "# print(f\"Le vecteur normalisé du mot 'player' est:\\n {normaliser(vecteur_player)}\\n\")\n",
    "print(f\"L'angle entre les deux vecteurs en degrés est environ {angle(vecteur_jeu, vecteur_player)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f39f5",
   "metadata": {},
   "source": [
    "\n",
    "&rarr; Ces tokens échangent ensuite entre eux pour décider quels éléments du contexte influent sur la **sémantique appropriée** de certains mots de la phrase. Par exemple, quand on dit \"graver en mémoire\", c'est un verbe 'graver' qui diffère de celui dans \"graver dans de la pierre\". On peut appeler cette étape \"*bloc d'Attention*\".\n",
    "\n",
    "&rarr; Ensuite, ces vecteurs passent par une étape au nom barbare de *perceptron multi-couches* ... le procédé mathématique exact est un peu compliqué à vulgariser donc on va juste dire que c'est une manière pour le modèle de catégoriser et mettre à jour chaque vecteur de token, comme s'il lui posait à la suite pleins de questions et qu'il prenait en compte les réponses pour affiner le vecteur.\n",
    "\n",
    "&rarr; La suite est une répétition d'alternances entre **blocs d'attention** et **perceptrons multi-couches**, jusqu'à la dernière itération où le dernier vecteur de la séquence sera donc le résultat combiné de tout ce passage.\n",
    "\n",
    "&rarr; En opérant sur ce vecteur, on obtient une **distribution de probabilités** de tous les *tokens* possibles en sortie (donc tous les mots/segments de texte qui peuvent se placer linguistiquement dans la continuité de notre texte initial). En gros, on obtient une liste des mots qui pourront **probablement** le mieux compléter la suite du texte donné.\n",
    "\n",
    "Mais tout ¢a, c'est pour obtenir **un** mot. Ainsi, on peut recommencer une fois de plus en ajoutant notre mot nouvellement obtenu dans le contexte de l'itération suivante afin d'obtenir la suite. Et on peut recommencer encore et encore...\n",
    "\n",
    "Enfin, pour obtenir un ***Chatbot*** (pour faire simple, ChatGPT et équivalent) depuis ce modèle, en fait c'est tout bête voire assez marrant: On appose, en préfixe du prompt de l'utilisateur, un *contexte caché* où on précise que ce qui suit est une discussion entre une IA et son utilisateur. Donc, le prompt de l'utilisateur sera donné à l'IA sous la forme du début d'un dialogue. De là, le chatbot va en fait prédire mot à mot la réponse la plus probable que ferait un assistant IA dans le dialogue.\n",
    "\n",
    "SUITE A COMPLETER SUR LE PRE-TRAINING / FINE-TUNING. C'est pas fini..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c2208f",
   "metadata": {},
   "source": [
    "- ***TODO***\n",
    "\n",
    "mécanisme d'attention\n",
    "\n",
    "embedding / unembedding\n",
    "\n",
    "context size et autre\n",
    "\n",
    "Générative ≠ Créative -> Dépend du data set \n",
    "d'apprentissage; Il établit ses pondérations par rapport à ce qu'il a appris.\n",
    "\n",
    "Biais d'apprentissage ---> biais de confirmation venant du fine-tuning, et biais du dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e87915b",
   "metadata": {},
   "source": [
    "## Limites des LLMs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a0ba48",
   "metadata": {},
   "source": [
    "## Observer les pondérations que l'IA donne aux tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db54b38e",
   "metadata": {},
   "source": [
    "Pour observer la pondération que l'IA attribue aux tokens pour comprendre le contexte, on peut utiliser l'outil [*interpreto*](https://pypi.org/project/interpreto/), qui a pour ambition de rendre les LLMs plus transparents et de trouver une manière de mesurer leur fiabilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35359b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install interpreto"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
