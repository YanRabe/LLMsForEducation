{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b241f299",
   "metadata": {},
   "source": [
    "# Etude de cas de l'utilisation des IA Génératives (LLMs) dans l'enseignement supérieur de la science du numérique.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a41478",
   "metadata": {},
   "source": [
    "## Préambule\n",
    "\n",
    "Les IA génératives telles que ChatGPT, DeepSeek, ou Claude Sonnet sont indéniablement devenues ces dernières années des outils très prominents dans le quotidien numérique des étudiants. Étant des outils très puissants, il est essentiel de savoir s'en servir convenablement pour en tirer le maximum. Cela demande en premier lieu d'*au moins* comprendre dans les très (très) grandes lignes leur principe de fonctionnement et ce que la technologie implique, mais aussi leurs potentiel et limites actuelles. Cela évitera notamment ce genre de remarques:\n",
    "\n",
    "<div>\n",
    "    <center>\n",
    "    <img src=\"illustrative_tweet.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principe de l'IA Générative\n",
    "\n",
    "Pour faire très (très) simple en omettant toutes les maths derrière:\n",
    "\n",
    "&rarr; un **modèle de langage** comme **GPT 4.1** (≠ ChatGPT!) va, à partir d'un texte en entrée (un contexte en fait), découper ce texte en morceaux/~mots appelés *tokens* et leur attribuer un vecteur dans un espace de très grande dimension (beaucoup d'axes). \n",
    "\n",
    "&rarr; Ce vecteur déterminera d'une certaine manière le sens \"*sémantique*\" d'un token, et ainsi les autres mots de sens similaires auront des vecteurs proches dans l'espace. Les valeurs qui caractérisent ces vecteurs sont choisies aléatoirement puis apprises lors de l'**apprentissage** du modèle; une étape antérieure où on apprend au modèle à comparer et donc catégoriser les entrées qu'on lui donne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1804d558",
   "metadata": {},
   "source": [
    "Voici par exemple une manière d'observer des vecteurs de tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fed032",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1655012093.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install gensim\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#à ne lancer qu'une fois et à installer sur Python <=3.12 !\n",
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a528ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.1474e+00,  1.1811e+00,  7.4556e-01, -5.9101e-02,  5.0499e-01,\n",
       "       -7.0449e-01, -3.2136e-01, -4.5390e-01, -4.5763e-01, -7.5341e-01,\n",
       "       -3.3511e-01, -2.4975e-02, -5.0192e-01,  6.3773e-01, -8.3059e-01,\n",
       "        8.3565e-01, -2.4701e-01,  3.2421e-01, -1.1103e+00, -2.1335e-02,\n",
       "        6.8717e-01, -3.9340e-01, -1.6390e+00, -5.0493e-01, -1.6684e-01,\n",
       "       -6.7649e-01, -3.1798e-01,  8.8503e-01, -3.1552e-02, -1.5608e-01,\n",
       "        1.9805e+00, -1.1870e+00,  8.3342e-01, -1.8369e-01, -2.6691e-01,\n",
       "        1.1619e-01,  1.1023e+00, -3.5937e-01,  2.5015e-02, -4.0615e-02,\n",
       "        3.0681e-01, -4.1076e-01,  8.4586e-02,  2.2475e-01, -5.0955e-01,\n",
       "        6.5819e-01, -1.2432e-01, -1.4039e+00,  1.6178e-04, -5.2529e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "model = gensim.downloader.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "model[\"tower\"] #Exemple avec le vecteur du mot \"tour\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f39f5",
   "metadata": {},
   "source": [
    "\n",
    "&rarr; Ces tokens échangent ensuite entre eux pour décider quels éléments du contexte influent sur la sémantique appropriée de certains mots de la phrase. Par exemple, quand on dit \"graver en mémoire\", c'est un verbe 'graver' qui diffère de celui dans \"graver dans de la pierre\". On peut appeler cette étape \"*bloc d'Attention*.\n",
    "\n",
    "&rarr; Ensuite, ces vecteurs passent par une étape au nom barbare de *perceptron multi-couches* ... le procédé mathématique exact est un peu compliqué à vulgariser donc on va juste dire que c'est une manière pour le modèle de catégoriser et mettre à jour chaque vecteur de token, comme s'il lui posait à la suite pleins de questions et qu'il prenait en compte les réponses pour affiner le vecteur.\n",
    "\n",
    "&rarr; La suite est une répétition d'alternances entre blocs d'attention et perceptrons multi-couches, jusqu'à la dernière itération où le dernier vecteur de la séquence sera donc le résultat combiné de tout ce passage.\n",
    "\n",
    "&rarr; En opérant sur ce vecteur, on obtient une distribution de probabilités de tous les *tokens* possibles en sortie (donc tous les mots/segments de texte qui peuvent se placer linguistiquement dans la continuité de notre texte initial). En gros, on obtient une liste des mots qui pourront **probablement** le mieux compléter la suite du texte donné.\n",
    "\n",
    "Mais tout ¢a, c'est pour obtenir **un** mot. Ainsi, on peut recommencer une nouvelle fois en ajoutant notre mot nouvellement obtenu dans le contexte de l'itération suivante afin d'obtenir la suite. Et on peut recommencer encore et encore...\n",
    "\n",
    "Enfin, pour obtenir un Chatbot (un point de départ simple vers ChatGPT et équivalent) depuis ce modèle, en fait c'est tout bête voire assez marrant: On appose, en préfixe du prompt de l'utilisateur, un contexte caché où on précise que ce qui suit est une discussion entre une IA et son utilisateur. Donc, le prompt de l'utilisateur sera donnée à l'IA sous la forme du début d'un dialogue. De là, le chatbot va en fait prédire mot à mot la réponse la plus probable que ferait un assistant IA dans le dialogue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c2208f",
   "metadata": {},
   "source": [
    "mécanisme d'attention\n",
    "embedding / unembedding\n",
    "context size et autre"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
